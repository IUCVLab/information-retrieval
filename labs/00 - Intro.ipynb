{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95edd0c6-b44e-40d6-9e9c-48f42ddd5207",
   "metadata": {},
   "source": [
    "# 0. Installing course dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ae1d575-f278-450f-aa6f-d99e075a79e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: annoy~=1.17.0 in c:\\anaconda\\lib\\site-packages (from -r ../requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: bs4~=0.0.1 in c:\\anaconda\\lib\\site-packages (from -r ../requirements.txt (line 3)) (0.0.1)\n",
      "Requirement already satisfied: feedparser~=6.0.8 in c:\\anaconda\\lib\\site-packages (from -r ../requirements.txt (line 4)) (6.0.8)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (from bs4~=0.0.1->-r ../requirements.txt (line 3)) (4.9.3)\n",
      "Requirement already satisfied: sgmllib3k in c:\\anaconda\\lib\\site-packages (from feedparser~=6.0.8->-r ../requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4->bs4~=0.0.1->-r ../requirements.txt (line 3)) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7bedc-d559-437e-b106-12c860acfbab",
   "metadata": {},
   "source": [
    "# 1. Touching the Internet\n",
    "\n",
    "Solve the following task. Download [this page](https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt), and save it to the file with the name derived from the URL. File with another URL should not be save into the file with this name. E.g. [this file](https://github.com/IUCVLab/information-retrieval/blob/main/datasets/facts.txt).\n",
    "\n",
    "Ref: [requests](https://docs.python-requests.org/en/latest/) library is cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639fb2d7-d577-4ae6-beb4-2487213024cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
    "\n",
    "# TODO: download and save to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7601aaf-3534-42cd-bb33-665d2d92c79d",
   "metadata": {},
   "source": [
    "# 2. Parsing different formats\n",
    "\n",
    "Most probably, if you meet something in Internet, this is: binary, plain text, XML, or json. XML also splits into xHTML, RSS, Atom, SOAP, XML-RPC, ... . Your task is to learn, how to process different formats.\n",
    "\n",
    "## 2.1. JSON\n",
    "\n",
    "In [the given file](http://sprotasov.ru/data/postnauka.txt) there is valid json. Parse it and print all video URLs, which have `computer science` tag. Use built-in features of `requests`, or just a `json` library ([ref](https://docs.python.org/3/library/json.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c2c18-b896-40f9-bac9-2fefc5027da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# TODO. Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97530a0-46d4-47e3-a7bb-ca479680007d",
   "metadata": {},
   "source": [
    "## 2.2. HTML\n",
    "\n",
    "For a given GitHub repository extract logins of (advanced task) names of the contributors. [bs4](https://beautiful-soup-4.readthedocs.io/en/latest/) will help you to do the job.\n",
    "\n",
    "I can recommend to use CSS or XPath selectors. Sidebar elements are `div`'s with `BorderGrid-cell` class. Inside the one with contributors list there should be `ul` list with `d-flex` class. My personal recommendation is to use `css selectors`, which are [documented here](https://beautiful-soup-4.readthedocs.io/en/latest/#css-selectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67db473-a8f0-414c-adbc-ce0e9e4710bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "account = 'IUCVLab'\n",
    "repo = 'information-retrieval'\n",
    "url = f\"https://github.com/{account}/{repo}\"\n",
    "\n",
    "print(url)\n",
    "\n",
    "# TODO. Your code here should parse HTML source page and find contributors of the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708766b-0db3-4062-87a1-9ba96c60440b",
   "metadata": {},
   "source": [
    "# 2.3. RSS feed\n",
    "\n",
    "A lot of information is already organized in typed XML documents. Podcasts are just RSS feed. Parse [the feed of this podcast](http://sprotasov.ru/podcast/rss.xml) and print out the time span between the first and the last episodes. Use [`feedparser` for this](https://waylonwalker.com/parsing-rss-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2090f810-d706-4bb2-8c85-d485a48432a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wed, 21 July 2021 15:36:00 +0300'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "rss = 'http://sprotasov.ru/podcast/rss.xml'\n",
    "# feedparser.parse(rss) \n",
    "\n",
    "# TODO: complete the code to compute the time span of all the episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a63b1-c106-4a13-8e55-9240e9c8418f",
   "metadata": {},
   "source": [
    "# 3. Solving simple information retrieval task\n",
    "\n",
    "According to the name, `information retrieval` is the discipline, which helps retrieves information (from unstructured sources). Thus, we will retrieve some information from [this news article](https://www.bbc.com/news/world-us-canada-59944889). Your task is to write a code, which will answer the question: **How many people die every day in the US waiting for a transplant?** Write flexible enough code. Test yourself by changing the link to [this one](https://www.americantransplantfoundation.org/about-transplant/facts-and-myths/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7660c706-371b-4050-aede-e4b3e4014ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://www.bbc.com/news/world-us-canada-59944889'\n",
    "question = 'How many people die every day in the US waiting for a transplant?'\n",
    "\n",
    "# TODO. Impress me!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
