{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you are going to implement a standard document processing pipeline and then build a simple search engine based on it:\n",
    "- starting from crawling documents, \n",
    "- then building an inverted index,\n",
    "- answering queries using this index,\n",
    "- and organizing it as a simple web server.\n",
    "\n",
    "Second part is devoted to spellchecking.\n",
    "\n",
    "# 1. Building inverted index and answering queries\n",
    "\n",
    "## 1.1. Preprocessing\n",
    "\n",
    "First, we need a unified approach to documents preprocessing. Implement a class responsible for that. Complete the code for given functions (most of them are just one-liners) and make sure you pass the tests. Make use of `nltk` library or any other you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = {'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "                      'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with'}\n",
    "        self.ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        #TODO word tokenize text using nltk lib\n",
    "        return ['one', 'two', 'three']\n",
    "\n",
    "    \n",
    "    def stem(self, word, stemmer):\n",
    "        #TODO stem word using provided stemmer\n",
    "        return 'stemmed_word'\n",
    "\n",
    "    \n",
    "    def is_apt_word(self, word):\n",
    "        #TODO check if word is appropriate - not a stop word and isalpha, \n",
    "        # i.e consists of letters, not punctuation, numbers, dates\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        #TODO combine all previous methods together: tokenize lowercased text \n",
    "        # and stem it, ignoring not appropriate words\n",
    "        return ['one', 'two', 'three']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Preprocessor()\n",
    "text = 'To be, or not to be, that is the question'\n",
    "\n",
    "assert prep.tokenize(text) == ['To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question']\n",
    "assert prep.stem('retrieval', prep.ps) == 'retriev'\n",
    "assert prep.is_apt_word('qwerty123') is False\n",
    "assert prep.preprocess(text) == ['or', 'not', 'question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawling and Indexing\n",
    "\n",
    "### 1.2.1. Base classes\n",
    "\n",
    "Here are some base classes we will need for writing our indexer. The code from the last lab's solution is given, but note that you will need to change some of it, namely, the `parse` function. The reason is it always makes complete parsing, which we want to avoid when we only need links, for example, or a specific portion of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "\n",
    "class Document:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def download(self):\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            if response.status_code == 200:\n",
    "                self.content = response.content\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def persist(self, path):\n",
    "        with open(os.path.join(path, quote(self.url).replace('/', '_')), 'wb') as f:\n",
    "            f.write(self.content)\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "\n",
    "    def normalize(self, href):\n",
    "        if href is not None and href[:4] != 'http':\n",
    "            href = urllib.parse.urljoin(self.url, href)\n",
    "        return href\n",
    "\n",
    "    def parse(self):\n",
    "        #TODO change this method\n",
    "        \n",
    "        def tag_visible(element):\n",
    "            if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "                return False\n",
    "            if isinstance(element, Comment):\n",
    "                return False\n",
    "            return True\n",
    "            \n",
    "        \n",
    "        model = BeautifulSoup(self.content)\n",
    "        \n",
    "        self.anchors = []\n",
    "        a = model.find_all('a')\n",
    "        for anchor in a:\n",
    "            href = self.normalize(anchor.get('href'))\n",
    "            text = anchor.text\n",
    "            self.anchors.append((text, href))\n",
    "                        \n",
    "        self.images = []\n",
    "        i = model.find_all('img')\n",
    "        for img in i:\n",
    "            href = self.normalize(img.get('src'))\n",
    "            self.images.append(href)\n",
    "        \n",
    "        texts = model.findAll(text=True)\n",
    "        visible_texts = filter(tag_visible, texts)  \n",
    "        self.text = u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Main class\n",
    "\n",
    "The main indexer logic is here. We organize it as a crawler generator that adds certain visited pages to inverted index and saves them on disk. \n",
    "\n",
    "- `crawl_generator_for_index` method crawles the given website doing BFS, starting from `source` within given `depth`. Considers only inner pages (of a form https://www.reuters.com/...) for visiting. To speed up, doesn't consider for visiting pages with content type other than html: '.pdf', '.mp3', '.avi', '.mp4', '.txt'. If encounters an article page (of a form https://www.reuters.com/article/...), saves its content in a file in `collection_path` folder and populates the inverted index calling `index_doc` method. When done, saves on disk three resulting dictionaries:\n",
    "    - `doc_urls`, `doc_id:url`\n",
    "    - `index`, `term:[collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]`\n",
    "    - `doc_lengths`, `doc_id:doc_length` \n",
    "\n",
    "    `limit` parameter is given for testing - if not `None`, break the loop when number of saved articles exceeds the `limit` and return without writing dictionaries to disk.\n",
    "    \n",
    "    \n",
    "- `index_doc` method parses and preprocesses the content of a `doc` and adds it to the inverted index. Also keeps track of document lengths in a `doc_lengths` dictionary.\n",
    "\n",
    "\n",
    "**Bonus task \\* (no penalty to skip)** In real industrial systems a crawler would pass the links to the dedicated service that would load their contents in a bunch of parallel threads. Implement such a service - get urls as inputs, load page contents in parallel and return filenames on disk, which are then processed by indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from queue import Queue\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class Indexer:\n",
    "\n",
    "    def __init__(self):      \n",
    "        # dictionaries to populate\n",
    "        self.doc_urls = {}        \n",
    "        self.index = {}\n",
    "        self.doc_lengths = {}\n",
    "        # preprocessor\n",
    "        self.prep = Preprocessor()\n",
    "        \n",
    "    \n",
    "    def crawl_generator_for_index(self, source, depth, collection_path=\"collection\", limit=None):        \n",
    "        #TODO generate url-s for visiting\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def index_doc(self, doc, doc_id):\n",
    "        #TODO add documents to index\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = Indexer()\n",
    "k = 1\n",
    "for c in indexer.crawl_generator_for_index(\"https://www.reuters.com/technology\", 2, \"test_collection\", 5):\n",
    "    print(k, c.url)\n",
    "    k+=1\n",
    "\n",
    "assert type(indexer.index) is dict\n",
    "assert type(indexer.index['reuter']) is list\n",
    "assert type(indexer.index['reuter'][0]) is int\n",
    "assert type(indexer.index['reuter'][1]) is tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. Building an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = Indexer()\n",
    "k = 1\n",
    "for c in indexer.crawl_generator_for_index(\"https://www.reuters.com/\", 3, \"docs_collection\"):\n",
    "    print(k, c.url)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5. Index statistics\n",
    "\n",
    "Load an index and print the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index, doc_lengths and doc_urls\n",
    "# with open('inverted_index.p', 'rb') as fp:\n",
    "#     index = pickle.load(fp)\n",
    "# with open('doc_lengths.p', 'rb') as fp:\n",
    "#     doc_lengths = pickle.load(fp)\n",
    "# with open('doc_urls.p', 'rb') as fp:\n",
    "#     doc_urls = pickle.load(fp)\n",
    "    \n",
    "    \n",
    "print('Total index length', len(index))\n",
    "print('\\nTop terms by number of documents they apperared in:')\n",
    "sorted_by_n_docs = sorted(index.items(), key=lambda kv: (len(kv[1]), kv[0]), reverse=True)\n",
    "print([(sorted_by_n_docs[i][0], len(sorted_by_n_docs[i][1])) for i in range(20)])\n",
    "print('\\nTop terms by overall frequency:')\n",
    "sorted_by_freq = sorted(index.items(), key=lambda kv: (kv[1][0], kv[0]), reverse=True)\n",
    "print([(sorted_by_freq[i][0], sorted_by_freq[i][1][0]) for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Answering query\n",
    "\n",
    "Now, given that we already have built the inverted index, it's time to utilize it for answering user queries. In this class there are two methods you need to implement:\n",
    "- `boolean_retrieval`, the simplest form of document retrieval which returns a set of documents such that each one contains all query terms. Returns a set of document ids. Refer to *ch.1* of the book for details;\n",
    "- `okapi_scoring`, Okapi BM25 ranking function - assigns scores to documents in the collection that are relevant to the user query. Returns a dictionary of scores, `doc_id:score`. Read about it in [Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25#The_ranking_function) and implement accordingly.\n",
    "\n",
    "Both methods accept `query` parameter in a form of a dictionary, `term:frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class QueryProcessing:\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_query(raw_query):\n",
    "        prep = Preprocessor()\n",
    "        # pre-process query the same way as documents\n",
    "        query = prep.preprocess(raw_query)\n",
    "        # count frequency\n",
    "        return Counter(query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def boolean_retrieval(query, index):\n",
    "        #TODO retrieve a set of documents containing all query terms\n",
    "        return {0, 1, 3}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def okapi_scoring(query, doc_lengths, index, k1=1.2, b=0.75):\n",
    "        #TODO retrieve relevant documents with scores\n",
    "        return {0: 0.32, 5: 1.17}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_lengths = {1: 20, 2: 15, 3: 10, 4:20, 5:30}\n",
    "test_index = {'x': [2, (1, 1), (2, 1)], 'y': [2, (1, 1), (3, 1)], 'z': [3, (2, 1), (4,2)]}\n",
    "\n",
    "\n",
    "test_query1 = QueryProcessing.prepare_query('x z')\n",
    "test_query2 = QueryProcessing.prepare_query('x y')\n",
    "\n",
    "\n",
    "assert QueryProcessing.boolean_retrieval(test_query1, test_index) == {2}\n",
    "assert QueryProcessing.boolean_retrieval(test_query2, test_index) == {1}\n",
    "okapi_res = QueryProcessing.okapi_scoring(test_query2, test_doc_lengths, test_index)\n",
    "assert all(k in okapi_res for k in (1, 2, 3))\n",
    "assert not any(k in okapi_res for k in (4, 5))\n",
    "assert okapi_res[1] > okapi_res[3] > okapi_res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Setting up a server\n",
    "\n",
    "**Bonus task \\* (no penaly if skipped)** Organize the resulting search engine as a web-service that gets a query from get-parameters and returns urls with scores as a `json` dictionary. Check its work in a browser of with curl, should look smth like this:\n",
    " \n",
    "`> curl localhost:8080/?q=some_query_text\n",
    "{ \"url1\" : 0.9, \"url2\": 0.8 }`\n",
    "\n",
    "You can use one of the following tools for this task: \n",
    "- https://www.acmesystems.it/python_http, \n",
    "- [http.server.ThreadingHTTPServer (3.7+)](https://docs.python.org/3/library/http.server.html#http.server.SimpleHTTPRequestHandler)\n",
    "- [Flask](https://pypi.org/project/Flask/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO write a web-service that answers queries using inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sorri not veri gud in inglish\n",
    "\n",
    "Have you ever googled someone's name without knowing exactly how it should be written? Were you ever reluctant to look up the correct spelling of a query you typed? Or just unable to type properly because of being in a rush? Modern search engines usually do a pretty good job in deciphering defective user input. In order to be able to do that, a good spell-checking mechanism should be incorporated into a search procedure. Today we will take one step further towards building a good search engine and work on tolerant retrieval with respect to user queries. We will consider two cases:\n",
    "\n",
    "1. User knows that he doesn't know the correct spelling OR he wants to get the results that follow some known pattern, so he uses so called wildcards - queries like `retr*val`;\n",
    "2. User doesn't know the correct spelling OR he doesn't care OR he's in a rush OR he expects his mistakes will be corrected OR your option, so he makes mistakes and we need to handle them using:\n",
    "\n",
    "    2.1. Simple spellchecker by Peter Norvig;\n",
    "    \n",
    "    2.2. Phonetic correction by means of Soundex algorithm;\n",
    "    \n",
    "    2.3. Trigrams with Jaccard coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Handling wildcards\n",
    "\n",
    "We will handle wildcard queries using k-grams. K-grams is a list of consecutive k chars in a string - i.e., for the word *'star'*, it will be '*\\$st*', '*sta*', '*tar*', and '*ar$*', if we take k=3. Take a look at [book](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf) *chapter 3.2.2* to understand how k-grams can help efficiently match a wildcard against dictionary words. Here we will only consider wildcards with star symbols (may be multiple).\n",
    "\n",
    "Notice that for building k-grams index, **we will need a vocabulary of original word forms** to compare words in user input to the vocabulary of \"correct\" words (think why inverted index which we built for stemmed words doesn't work here).   \n",
    "\n",
    "You need to implement the following:\n",
    "\n",
    "- `build_inverted_index_orig_forms` - creates inverted index of original world forms from `facts` list, which is already given to you.  \n",
    "    Output format: `term:[collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]`\n",
    "    \n",
    "\n",
    "- `build_k_gram_index` - creates k-gram index which maps every k-gram encountered in facts collection to a list of words containing this k-gram. Use the abovementioned inverted index of original words to construct this index.  \n",
    "    Output format: `'k_gram': ['word1_with_k_gram', 'word2_with_k_gram', ...]`\n",
    "    \n",
    "    \n",
    "- `generate_wildcard_options` - produce a list of vocabulary words matching given wildcard by intersecting postings of k-grams present in the wildcard (refer to *ch 3.2.2*). \n",
    "\n",
    "- `search_wildcard` - return list of facts that contain the words matching a wildcard query.\n",
    "\n",
    "\n",
    "We will use the dataset with curious facts for testing.\n",
    "\n",
    "### 2.1.1. Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "data_url = \"https://raw.githubusercontent.com/hsu-ai-course/hsu.ai/master/code/datasets/nlp/facts.txt\"\n",
    "local_filename, headers = urllib.request.urlretrieve(data_url)\n",
    "\n",
    "facts = []\n",
    "with open(local_filename) as fp:\n",
    "    for cnt, line in enumerate(fp):\n",
    "        facts.append(line.strip('\\n'))\n",
    "        \n",
    "print(*facts[-5:], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Implementation of search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "def build_inverted_index_orig_forms(documents):\n",
    "    #TODO build an inverted index of original word forms \n",
    "    # (without stemming, just word tokenized and lowercased)   \n",
    "    inverted_index = {}\n",
    "        \n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "def build_k_gram_index(inverted_index, k):\n",
    "    #TODO build index of k-grams for dictionary words. \n",
    "    # Padd with '$' ($word$) before splitting to k-grams    \n",
    "    k_gram_index = {}\n",
    "    \n",
    "    return k_gram_index\n",
    "\n",
    "\n",
    "def generate_wildcard_options(wildcard, k_gram_index, inverted_index):\n",
    "    #TODO for a given wildcard return all words matching it using k-grams\n",
    "    # refer to book chapter 3.2.2\n",
    "    # don't forget to pad wildcard with '$', when appropriate    \n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "def search_wildcard(wildcard, k_gram_index, index, docs):\n",
    "    #TODO retrive list of documnets (facts) that contain words matching wildcard\n",
    "           \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_orig_forms = build_inverted_index_orig_forms(facts)\n",
    "k_gram_index = build_k_gram_index(index_orig_forms, 3)\n",
    "\n",
    "wildcard = \"re*ed\"\n",
    "\n",
    "wildcard_options = generate_wildcard_options(wildcard, k_gram_index, index_orig_forms)\n",
    "print(wildcard_options)\n",
    "assert(len(wildcard_options) >= 3)\n",
    "\n",
    "wildcard_results = search_wildcard(wildcard, k_gram_index, index_orig_forms, facts)\n",
    "# some pretty printing\n",
    "for r in wildcard_results:\n",
    "    # highlight terms for visual evaluation\n",
    "    for term in wildcard_options:\n",
    "        r = re.sub(r'(' + term + ')', r'\\033[1m\\033[91m\\1\\033[0m', r, flags=re.I)\n",
    "    print(r)\n",
    "\n",
    "assert(len(wildcard_results) >=3)\n",
    "\n",
    "assert \"13. James Buchanan, the 15th U.S. president continuously bought slaves with his own money in order to free them.\" in search_wildcard(\"pres*dent\", k_gram_index, index_orig_forms, facts)\n",
    "assert \"40. 9 out of 10 Americans are deficient in Potassium.\" in search_wildcard(\"p*tas*um\", k_gram_index, index_orig_forms, facts)\n",
    "assert \"61. A man from Britain changed his name to Tim Pppppppppprice to make it harder for telemarketers to pronounce.\" in search_wildcard(\"*price\", k_gram_index, index_orig_forms, facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Handling typos\n",
    "\n",
    "### 2.2.1 Dataset \n",
    "\n",
    "Download github typo dataset from [here](https://github.com/mhagiwara/github-typo-corpus).\n",
    "Load it with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "dataset_file = \"github-typo-corpus.v1.0.0.jsonl\"\n",
    "\n",
    "dataset = []\n",
    "other_langs = set()\n",
    "\n",
    "with jsonlines.open(dataset_file) as reader:\n",
    "    for obj in reader:\n",
    "        for edit in obj['edits']:\n",
    "            if edit['src']['lang'] != 'eng':\n",
    "                other_langs.add(edit['src']['lang'])\n",
    "                continue\n",
    "\n",
    "            if edit['is_typo']:\n",
    "                src, tgt = edit['src']['text'], edit['tgt']['text']\n",
    "                if src.lower() != tgt.lower():\n",
    "                    dataset.append((edit['src']['text'], edit['tgt']['text']))\n",
    "                \n",
    "print(f\"Dataset size = {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore sample typos\n",
    "Please, explore the dataset. You may see, that this is\n",
    "- mostly markdown\n",
    "- some common mistakes with do/does\n",
    "- some just refer to punctuation typos (which we do not consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in dataset[1010:1020]:\n",
    "    print(f\"{pair[0]} => {pair[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Build a dataset vocabulary\n",
    "We will need it for Norvig's spellchecker as well as for estimating overall correction quality. Consider only word-level. Be carefull, there is markdown (e.g. \\`name\\`. \\[url\\]\\(http://url)) and comment symbols (\\#, //, \\*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sent):\n",
    "    # splits sentence to words, filtering out non-alphabetical terms\n",
    "    words = nltk.word_tokenize(sent)    \n",
    "    words_filtered = filter(lambda x: x.isalpha(), words)\n",
    "    return words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Counter()\n",
    "for pair in dataset:\n",
    "    for word in sent_to_words(pair[1].lower()):\n",
    "        vocabulary[word] += 1\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "print(list(islice(vocabulary.items(), 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Implement context-independent spellcheker ##\n",
    "\n",
    "0) Write code to compute editorial distance\n",
    "\n",
    "1) [Norvig's corrector](https://norvig.com/spell-correct.html)\n",
    "\n",
    "2) [Soundex](https://en.wikipedia.org/wiki/Soundex)\n",
    "\n",
    "3) Trigrams with Jaccard coefficient.\n",
    "\n",
    "#### 2.2.3.0. Editorial distance\n",
    "\n",
    "Frequently used distance measure between two character sequences. We will use this distance to sort Soundex search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_dist(s1, s2) -> int:\n",
    "    # TODO compute the Damerau-Levenshtein distance between two given strings (s1 and s2)\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "\n",
    "assert edit_dist(\"korrectud\", \"corrected\") == 2, \"Edit distance is computed incorrectly\"\n",
    "assert edit_dist(\"soem\", \"some\") == 1, \"Edit distance is computed incorrectly\"\n",
    "assert edit_dist(\"one\", \"one\") == 0, \"Edit distance is computed incorrectly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3.1. Norvig's spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_typo_norvig(word) -> str:\n",
    "    #TODO return best matching result for the word\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "\n",
    "assert fix_typo_norvig(\"korrectud\") == \"corrected\", \"Norvig's correcter doesn't work\"\n",
    "assert fix_typo_norvig(\"speling\") == \"spelling\", \"Norvig's correcter doesn't work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3.2. Soundex \n",
    "\n",
    "For cases when the exact spelling is unknown, phonetic algorithms such as Soundex can be very helpful - they allow user to type a word the way he thinks it should sound, and then suggest the corrrect version. Go through *chapter 3.4* to understand how Soundex algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_soundex_code(word):\n",
    "    #TODO implement Soundex algorithm, version from book chapter 3.4\n",
    "    # input word is already lowercased\n",
    "    # return Soundex 4-character code, like 'k450'\n",
    "        \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def build_soundex_index(dictionary):\n",
    "    #TODO build soundex index for dictionary words.\n",
    "    # dictionary is a vocabulary of original words\n",
    "    # output format: 'code1': ['word1_with_code1', 'word2_with_code1', ...]    \n",
    "    soundex_index = {}\n",
    "    \n",
    "    return soundex_index\n",
    "\n",
    "\n",
    "def fix_typo_soundex(word, soundex_index) -> list:\n",
    "    #TODO return words from vocabulary that match with result by soundex fingerprint\n",
    "    # ordered results by editorial distance\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "\n",
    "soundex_index = build_soundex_index(vocabulary)\n",
    "\n",
    "code1 = produce_soundex_code(\"britney\")\n",
    "code2 = produce_soundex_code(\"breatany\")\n",
    "print(code1, code2)\n",
    "assert code1 == code2\n",
    "\n",
    "print(fix_typo_soundex(\"enouhg\", soundex_index))\n",
    "assert \"enough\" in fix_typo_soundex(\"enouhg\", soundex_index), \"Assert soundex failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3.3. Trigrams with Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_typo_kgram(word, k_gram_index) -> list:\n",
    "    #TODO return best matches with respect to Jaccard index   \n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "\n",
    "k_gram_index_github = build_k_gram_index(vocabulary, 3)\n",
    "print(fix_typo_kgram(\"enouh\", k_gram_index_github)[:20])\n",
    "assert \"enough\" in fix_typo_kgram(\"enouh\", k_gram_index_github), \"Assert k-gram failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4. Estimate quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norvig, soundex, kgram = 0, 0, 0\n",
    "limit = 10000\n",
    "counter = limit\n",
    "for i, (src, target) in enumerate(dataset):\n",
    "    if i == limit:\n",
    "        break\n",
    "    words = sent_to_words(src.lower())\n",
    "    # word suspected for typos\n",
    "    sn, ss, sk = src.lower(), src.lower(), src.lower()\n",
    "    for word in words:\n",
    "        if word not in vocabulary and word.isalpha():\n",
    "            # top-1 accuracy\n",
    "            wn, ws, wk = fix_typo_norvig(word), \\\n",
    "                         fix_typo_soundex(word, soundex_index)[0], \\\n",
    "                         fix_typo_kgram(word, k_gram_index_github)[0]\n",
    "            sn = sn.replace(word, wn)\n",
    "            ss = ss.replace(word, ws)\n",
    "            sk = sk.replace(word, wk)\n",
    "    norvig += int(sn == target.lower())\n",
    "    soundex += int(ss == target.lower())\n",
    "    kgram += int(sk == target.lower())\n",
    "\n",
    "print(f\"Norvig accuracy ({norvig}) = {norvig / limit}\")\n",
    "print(f\"Soundex accuracy ({soundex}) = {soundex / limit}\")\n",
    "print(f\"k-gram accuracy ({kgram}) = {kgram / limit}\")\n",
    "\n",
    "# Norvig accuracy (2346) = 0.2346\n",
    "# Soundex accuracy (1673) = 0.1673\n",
    "# k-gram accuracy (1566) = 0.1566"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
